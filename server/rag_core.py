# æ–‡ä»¶ä½ç½®: server/rag_core.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.documents import Document

# â• æ–°å¢ï¼šå¼•å…¥ PDF åŠ è½½å™¨
from langchain_community.document_loaders import PyPDFLoader

load_dotenv()

# è¿™é‡Œçš„é€»è¾‘å’Œä½ ä¹‹å‰çš„ä¸€æ¨¡ä¸€æ ·ï¼Œåªæ˜¯å°è£…æˆäº†ç±»
class RAGService:
    def __init__(self, api_key, base_url):
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self.llm = ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model="deepseek-chat",
            temperature=0.1
        )
        self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        self.vector_store = None
    
    # 1. ä¿ç•™åŸæ¥çš„å­—ç¬¦ä¸²åˆå§‹åŒ–æ–¹æ³• (ä¸ºäº†å…¼å®¹)
    def init_from_text(self, text_content):
        text_splitter = CharacterTextSplitter(separator="\n", chunk_size=100, chunk_overlap=10)
        docs = [Document(page_content=x) for x in text_splitter.split_text(text_content)]
        self.vector_store = FAISS.from_documents(docs, self.embeddings)
        print("âœ… æ–‡æœ¬çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")

    # 2. â• æ–°å¢ï¼šä» PDF æ–‡ä»¶åˆå§‹åŒ–
    def add_pdf(self, file_path):
        print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
        # A. åŠ è½½ PDF
        loader = PyPDFLoader(file_path)
        docs = loader.load() # è¿™é‡Œä¼šæŠŠ PDF æ¯ä¸€é¡µè¯»å‡ºæ¥
        
        # B. åˆ‡åˆ† (æŠŠæ¯ä¸€é¡µå†åˆ‡ç¢ç‚¹ï¼Œæ–¹ä¾¿æ£€ç´¢)
        text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        split_docs = text_splitter.split_documents(docs)

        # C. å­˜å…¥å‘é‡åº“
        if self.vector_store is None:
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ï¼Œå°±æ–°å»ºåº“
            self.vector_store = FAISS.from_documents(split_docs, self.embeddings)
        else:
            # å¦‚æœåº“é‡Œå·²ç»æœ‰ä¸œè¥¿äº†ï¼Œå°±æŠŠæ–°ä¹¦â€œåŠ â€è¿›å»
            self.vector_store.add_documents(split_docs)
        
        print(f"âœ… PDF '{file_path}' å·²æˆåŠŸåŠ å…¥çŸ¥è¯†åº“ï¼")
    
    # ğŸ”´ ä¹Ÿå°±æ˜¯æŠŠåŸæ¥çš„ chat æ–¹æ³•æ”¹é€ æˆä¸‹é¢è¿™æ ·
    def chat_stream(self, question: str):
        if not self.vector_store:
            yield "çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼"
            return
            
        # 1. æ£€ç´¢ (å’Œä»¥å‰ä¸€æ ·)
        docs = self.vector_store.similarity_search(question, k=2)
        context = "\n".join([d.page_content for d in docs])
        
        prompt = f"å·²çŸ¥ä¿¡æ¯ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}\nè¯·æ ¹æ®å·²çŸ¥ä¿¡æ¯å›ç­”ã€‚"
        
        # 2. è°ƒç”¨ LLM (å¼€å¯æµå¼æ¨¡å¼!)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥å¾ªç¯ llm.streamï¼Œè€Œä¸æ˜¯ invoke
        for chunk in self.llm.stream(prompt):
            content = chunk.content
            if content:
                # yield å°±åƒæ˜¯â€œæŒ¤ç‰™è†â€ï¼ŒæŒ¤ä¸€ç‚¹å‡ºæ¥ç»™å¤–é¢
                yield content

# å®ä¾‹åŒ–ä¸€ä¸ªå…¨å±€å¯¹è±¡ä¾›å¤§å®¶è°ƒç”¨
rag_service = RAGService(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url=os.getenv("DEEPSEEK_BASE_URL")
)