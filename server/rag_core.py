# æ–‡ä»¶ä½ç½®: server/rag_core.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter  # â• æ–°å¢ï¼šé€’å½’åˆ‡åˆ†å™¨
from langchain_openai import OpenAIEmbeddings   # ğŸ‘ˆ åµŒå…¥æ¨¡å‹
from langchain_openai import ChatOpenAI         # ğŸ‘ˆ èŠå¤©æ¨¡å‹

# â• æ–°å¢ï¼šå¼•å…¥ PDF,word,excel åŠ è½½å™¨
from langchain_community.document_loaders import PyPDFLoader,Docx2txtLoader,UnstructuredExcelLoader

load_dotenv()

# è¿™é‡Œçš„é€»è¾‘å’Œä½ ä¹‹å‰çš„ä¸€æ¨¡ä¸€æ ·ï¼Œåªæ˜¯å°è£…æˆäº†ç±»
class RAGService:
    def __init__(self, api_key, base_url):
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self.llm = ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model="deepseek-chat",
            temperature=0.1
        )
        # ğŸ”´ 2. ä¿®æ”¹è¿™é‡Œï¼šæ¢å› HuggingFaceEmbeddings (æœ¬åœ°è¿è¡Œï¼Œå…è´¹ï¼Œç¨³å®š)
        # self.embeddings = OpenAIEmbeddings(...) âŒ åˆ æ‰æˆ–æ³¨é‡Šè¿™è¡Œ
        
        print("æ­£åœ¨åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹ (é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½)...")
        self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2") 
        # âœ… ä½¿ç”¨è¿™ä¸ªï¼å®ƒä¼šä¸‹è½½ä¸€ä¸ªå°æ¨¡å‹åˆ°ä½ ç”µè„‘ä¸Šï¼Œä¸ç”¨è”ç½‘ä¹Ÿèƒ½è·‘
        self.vector_store_path = "faiss_index" # ğŸ’¾ ç´¢å¼•ä¿å­˜è·¯å¾„
        self.vector_store = self._load_vector_store() # ğŸ”„ å¯åŠ¨æ—¶å°è¯•åŠ è½½
        
    # ğŸ”„ å†…éƒ¨æ–¹æ³•ï¼šå°è¯•ä»ç¡¬ç›˜åŠ è½½ç´¢å¼•
    def _load_vector_store(self):
        if os.path.exists(self.vector_store_path):
            try:
                # allow_dangerous_deserialization=True æ˜¯ä¸ºäº†åŠ è½½æœ¬åœ° pickle æ–‡ä»¶
                vs = FAISS.load_local(self.vector_store_path, self.embeddings, allow_dangerous_deserialization=True)
                print("âœ… [RAG] æˆåŠŸåŠ è½½æœ¬åœ°ç´¢å¼•ï¼")
                return vs
            except Exception as e:
                print(f"âš ï¸ [RAG] åŠ è½½ç´¢å¼•å¤±è´¥ï¼Œå°†é‡å»º: {e}")
                return None
        return None

    # ğŸ’¾ å†…éƒ¨æ–¹æ³•ï¼šä¿å­˜ç´¢å¼•åˆ°ç¡¬ç›˜
    def _save_vector_store(self):
        if self.vector_store:
            self.vector_store.save_local(self.vector_store_path)
            print("ğŸ’¾ [RAG] ç´¢å¼•å·²ä¿å­˜åˆ°æœ¬åœ°")
    
    # 1. ä¿ç•™åŸæ¥çš„å­—ç¬¦ä¸²åˆå§‹åŒ–æ–¹æ³• (ä¸ºäº†å…¼å®¹)
    def init_from_text(self, text_content):
        text_splitter = CharacterTextSplitter(separator="\n", chunk_size=100, chunk_overlap=10)
        docs = [Document(page_content=x) for x in text_splitter.split_text(text_content)]
        self.vector_store = FAISS.from_documents(docs, self.embeddings)
        print("âœ… æ–‡æœ¬çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")

    # ğŸ”„ [é‡æ„] è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨é€šç”¨æ–¹æ³•ï¼Œä¸ç®¡ä»€ä¹ˆæ–‡ä»¶ï¼Œè¯»å‡ºæ¥åéƒ½èµ°è¿™å¥—æµç¨‹
    def _proccess_and_save(self, docs,file_path):
        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} é¡µ æ–‡æ¡£")
        
        # ç»Ÿä¸€ä½¿ç”¨é…ç½®å¥½çš„åˆ‡åˆ†å™¨
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        split_docs = splitter.split_documents(docs)
        print(f"âœ… åˆ‡åˆ†æˆ {len(split_docs)} çŸ¥è¯†ç‰‡æ®µ")
        
        if self.vector_store:
            self.vector_store.add_documents(split_docs)
            print("âœ… å·²ç»è¿½åŠ åˆ°ç°æœ‰çŸ¥è¯†åº“")
        else:
            self.vector_store = FAISS.from_documents(split_docs, self.embeddings)
            print("âœ… åˆå§‹åŒ–äº†æ–°çš„çŸ¥è¯†åº“")  
            
        self._save_vector_store()
        print(f"âœ… æ–‡ä»¶ '{os.path.basename(file_path)}' å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ï¼") 
        
    # 2. æ–°å¢ï¼šæ·»åŠ  PDF æ–‡ä»¶åˆ°çŸ¥è¯†åº“,è°ƒç”¨ä¸Šé¢çš„é€šç”¨æ–¹æ³•    
    def add_pdf(self, file_path):
        # try:
        #     #åŠ è½½ PDF æ–‡ä»¶
        #     loader = PyPDFLoader(file_path)
        #     docs = loader.load()
        #     print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} é¡µ PDF")
            
        #     #åˆ‡åˆ†æ–‡æ¡£
        #     # ğŸ’¡ çŸ¥è¯†ç‚¹ï¼šchunk_size è¶Šå°ï¼Œæ£€ç´¢è¶Šç²¾å‡†ï¼Œä½†ä¸¢å¤±ä¸Šä¸‹æ–‡ï¼›è¶Šå¤§ï¼Œä¸Šä¸‹æ–‡å®Œæ•´ï¼Œä½†å™ªéŸ³å¤šã€‚
        #     splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        #     split_docs = splitter.split_documents(docs)
        #     print(f"âœ… åˆ‡åˆ†æˆ {len(split_docs)} çŸ¥è¯†ç‰‡æ®µ")
            
        #     #åŠ å…¥åˆ°å‘é‡åº“(å†…å­˜ä¸­)
        #     if self.vector_store:
        #         self.vector_store.add_documents(split_docs)
        #         print("âœ… å·²ç»è¿½åŠ åˆ°ç°æœ‰çŸ¥è¯†åº“")
        #     else:
        #         self.vector_store = FAISS.from_documents(split_docs, self.embeddings)
        #         print("âœ… åˆå§‹åŒ–äº†æ–°çš„çŸ¥è¯†åº“")
                
        #     #ä¿å­˜åˆ°æœ¬åœ°
        #     self._save_vector_store()
        #     print(f"âœ… æ–‡ä»¶ '{os.path.basename(file_path)}' å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ï¼")
        # except Exception as e:
        #     print(f"âŒ æ·»åŠ æ–‡ä»¶å¤±è´¥: {e}")
        #     raise e # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†    
        print(f"æ­£åœ¨å¤„ç† PDF æ–‡ä»¶: {file_path}")
        try:
            # åŠ è½½ PDF æ–‡ä»¶
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            self._proccess_and_save(docs,file_path)
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡ä»¶å¤±è´¥: {e}")
            raise e  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
         
    # â• æ–°å¢ï¼šæ·»åŠ  Word æ–‡ä»¶åˆ°çŸ¥è¯†åº“,è°ƒç”¨ä¸Šé¢çš„é€šç”¨æ–¹æ³•
    def add_word(self, file_path):
        print(f"æ­£åœ¨å¤„ç† Word æ–‡ä»¶: {file_path}")
        try:
            # åŠ è½½ Word æ–‡ä»¶
            loader = Docx2txtLoader(file_path)
            docs = loader.load()
            self._proccess_and_save(docs,file_path)
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡ä»¶å¤±è´¥: {e}")
            raise e  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
    
    # â• æ–°å¢ï¼šæ·»åŠ  Excel æ–‡ä»¶åˆ°çŸ¥è¯†åº“,è°ƒç”¨ä¸Šé¢çš„é€šç”¨æ–¹æ³•
    def add_excel(self, file_path):
        print(f"æ­£åœ¨å¤„ç† Excel æ–‡ä»¶: {file_path}")
        try:
            # åŠ è½½ Excel æ–‡ä»¶
            #mode="elements" æŒ‰è¡ŒåŠ è½½ï¼Œæ›´é€‚åˆè¡¨æ ¼
            loader = UnstructuredExcelLoader(file_path,mode="elements")
            docs = loader.load()
            self._proccess_and_save(docs,file_path)
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡ä»¶å¤±è´¥: {e}")
            raise e  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
              
    #ğŸ†• æ–°å¢ï¼šåˆ é™¤æ–‡ä»¶ï¼ˆé€šè¿‡é‡å»ºç´¢å¼•çš„æ–¹å¼ï¼Œè¿™æ˜¯æœ€ç®€å•ç¨³å¦¥çš„æ–¹æ³•ï¼‰
    def delete_file(self, filename):
        # if not self.vector_store:
        #     print("âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— éœ€åˆ é™¤")
        #     return
        
        # # 1. è·å–æ‰€æœ‰æ–‡æ¡£
        # all_docs = self.vector_store.documents
        
        # # 2. è¿‡æ»¤æ‰è¦åˆ é™¤çš„æ–‡ä»¶å¯¹åº”çš„æ–‡æ¡£
        # remaining_docs = [doc for doc in all_docs if not doc.metadata.get("source", "").endswith(filename)]
        
        # # 3. é‡å»ºç´¢å¼•
        # self.vector_store = FAISS.from_documents(remaining_docs, self.embeddings)
        # print(f"âœ… æ–‡ä»¶ '{filename}' å·²ä»çŸ¥è¯†åº“ä¸­åˆ é™¤ï¼")
        
        # # 4. ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
        # self._save_vector_store()
        
        # 1. ç®€å•ç²—æš´æ–¹æ¡ˆï¼šæ¸…ç©ºå†…å­˜é‡Œçš„ç´¢å¼•
        self.vector_store = None
        
        # 2. é‡æ–°æ‰«æ uploads æ–‡ä»¶å¤¹é‡Œçš„æ‰€æœ‰ PDF é‡å»º
        # (çœŸå®ç”Ÿäº§ç¯å¢ƒä¼šç”¨ delete by IDï¼Œä½† FAISS ç®€å•ç‰ˆä¸æ”¯æŒï¼Œé‡å»ºæœ€ç¨³)
        uploads_dir = "uploads"
        if os.path.exists(uploads_dir):
            files = [f for f in os.listdir(uploads_dir) if f.endswith(".pdf") and f != filename]
            
            # å¦‚æœè¿˜æœ‰å…¶ä»–æ–‡ä»¶ï¼Œå°±é‡æ–°æŠŠå®ƒä»¬åŠ è¿›å»
            for f in files:
                self.add_pdf(os.path.join(uploads_dir, f))
                
        # å¦‚æœåˆ å…‰äº†ï¼Œè®°å¾—æŠŠæœ¬åœ°çš„ç´¢å¼•æ–‡ä»¶ä¹Ÿåˆ äº†
        if not self.vector_store and os.path.exists(self.vector_store_path):
            import shutil
            shutil.rmtree(self.vector_store_path)
    
    # ğŸ”´ ä¹Ÿå°±æ˜¯æŠŠåŸæ¥çš„ chat æ–¹æ³•æ”¹é€ æˆä¸‹é¢è¿™æ ·
    def chat_stream(self, question: str):
        if not self.vector_store:
            yield "çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼"
            return
            
        # 1. æ£€ç´¢ (å’Œä»¥å‰ä¸€æ ·)
        docs = self.vector_store.similarity_search(question, k=2)
        context = "\n".join([d.page_content for d in docs])
        
        prompt = f"å·²çŸ¥ä¿¡æ¯ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}\nè¯·æ ¹æ®å·²çŸ¥ä¿¡æ¯å›ç­”ã€‚"
        
        # 2. è°ƒç”¨ LLM (å¼€å¯æµå¼æ¨¡å¼!)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥å¾ªç¯ llm.streamï¼Œè€Œä¸æ˜¯ invoke
        for chunk in self.llm.stream(prompt):
            content = chunk.content
            if content:
                # yield å°±åƒæ˜¯â€œæŒ¤ç‰™è†â€ï¼ŒæŒ¤ä¸€ç‚¹å‡ºæ¥ç»™å¤–é¢
                yield content

# å®ä¾‹åŒ–ä¸€ä¸ªå…¨å±€å¯¹è±¡ä¾›å¤§å®¶è°ƒç”¨
rag_service = RAGService(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url=os.getenv("DEEPSEEK_BASE_URL")
)